{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.10.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "grid_size = 5  # Grid size for player and enemies (positions range from 0 to 4)\n",
    "num_enemies = 4  # Number of enemies\n",
    "visited_states = 2  # Visited status can be 0 or 1\n",
    "wall_states = 2  # Wall status can be 0 (no wall) or 1 (wall)\n",
    "\n",
    "movements = np.array([\n",
    "    [0, 1],   # Move up\n",
    "    [1, 0],   # Move right\n",
    "    [-1, 0],  # Move left\n",
    "    [0, -1]   # Move down\n",
    "])\n",
    "\n",
    "# Generate walls: 0 for no wall, 1 for wall\n",
    "\n",
    "state_action_log = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Shape: (2, 2, 5, 5, 5, 5, 5, 5, 5, 5, 2)\n",
      "Estimated Value Grid Shape: (2, 2, 5, 5, 5, 5, 5, 5, 5, 5)\n",
      "Reward States Shape: (5, 5)\n"
     ]
    }
   ],
   "source": [
    "base_shape = [ visited_states, wall_states]\n",
    "enemy_shape = [grid_size, grid_size] * num_enemies  # Each enemy has x and y positions\n",
    "full_shape = base_shape + enemy_shape\n",
    "\n",
    "# # Generate estimated_value_grid filled with zeros\n",
    "estimated_value_grid = np.zeros(full_shape, dtype=float)\n",
    "\n",
    "# # Generate random indices into the movements array\n",
    "policy_indices = np.random.randint(0, 4, size=full_shape)\n",
    "\n",
    "# # Create the policy array by indexing into movements\n",
    "policy = movements[policy_indices]\n",
    "\n",
    "# # Now, policy has shape full_shape + (2,), where the last dimension stores (dx, dy)\n",
    "print(\"Policy Shape:\", policy.shape)  # For verification\n",
    "\n",
    "# # Generate reward_states: 40x40 grid of random 1s and 2s\n",
    "reward_states = np.random.choice([1, 2], size=(grid_size, grid_size))\n",
    "\n",
    "# # Output shapes for verification\n",
    "print(\"Estimated Value Grid Shape:\", estimated_value_grid.shape)\n",
    "print(\"Reward States Shape:\", reward_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = {}\n",
    "estimated_value_grid={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_maze_optimized(maze, s):\n",
    "    x, y = s\n",
    "    rows, cols = len(maze), len(maze[0])\n",
    "    half_grid = 5 // 2\n",
    "\n",
    "    partition = [\n",
    "        [\n",
    "            maze[(x + dx) % rows][(y + dy) % cols]\n",
    "            for dy in range(-half_grid, half_grid + 1)\n",
    "        ]\n",
    "        for dx in range(-half_grid, half_grid + 1)\n",
    "    ]\n",
    "    return np.array(partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_relative_positions(maze_size, player_pos, enemy_positions, grid_size=5):\n",
    "    x, y = player_pos\n",
    "    rows, cols = maze_size  # Maze dimensions\n",
    "    half_grid = grid_size // 2\n",
    "\n",
    "    relative_positions = []\n",
    "\n",
    "    for ex, ey in enemy_positions:\n",
    "        # Compute differences considering wrap-around\n",
    "        dx = (ex - x + cols) % cols\n",
    "        if dx > cols // 2:\n",
    "            dx -= cols\n",
    "\n",
    "        dy = (ey - y + rows) % rows\n",
    "        if dy > rows // 2:\n",
    "            dy -= rows\n",
    "\n",
    "        # Check if enemy is within the local grid\n",
    "        if -half_grid <= dx <= half_grid and -half_grid <= dy <= half_grid:\n",
    "            # Map to local grid coordinates (0 to 4)\n",
    "            local_x = int(dx + half_grid)\n",
    "            local_y = int(dy + half_grid)\n",
    "            relative_positions.append((local_x, local_y))\n",
    "    return relative_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_collision(current_state, enemies):\n",
    "    x, y = current_state\n",
    "    for enemy in enemies:\n",
    "        ex, ey = enemy[\"pos\"]\n",
    "        if (x, y) == (ex, ey):\n",
    "            return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_wall_ahead(current_state, action, reward_maze):\n",
    "    x, y = current_state\n",
    "    dx, dy = action\n",
    "    maze_rows, maze_cols = len(reward_maze),len(reward_maze[0])\n",
    "    nextx, nexty = (x + dx) % maze_rows, (y + dy) % maze_cols\n",
    "    return reward_maze[nextx][nexty] == 1  # Returns True if the next cell is a wall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_actions(current_state, reward_maze):\n",
    "    x, y = current_state\n",
    "    maze_rows, maze_cols = len(reward_maze),len(reward_maze[0])\n",
    "    valid_actions = []\n",
    "    for dx, dy in movements:\n",
    "        nextx, nexty = (x + dx) % maze_rows, (y + dy) % maze_cols\n",
    "        if reward_maze[nextx][nexty] != 1:  # Not a wall\n",
    "            valid_actions.append((dx, dy))\n",
    "    return valid_actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy_estimated_vals(epsilon,old_indices_s,policy, prev_player_pos,estimated_value_grid, og_enemy_positions, reward_maze,pack):\n",
    "    move_values=[(1,0),(-1,0),(0,-1),(0,1)]\n",
    "    maze_rows, maze_cols = len(reward_maze),len(reward_maze[0])\n",
    "    x,y=prev_player_pos\n",
    "    e1=optimized_relative_positions(\n",
    "        (maze_rows, maze_cols), ((x+1)%maze_rows,y), og_enemy_positions, grid_size=grid_size\n",
    "    )\n",
    "    f1 = int(reward_maze[(x+1)%maze_rows][y] == 2)\n",
    "    w1 = int(reward_maze[(x+1)%maze_rows][y] == 1)\n",
    "    max_enemies_in_local_grid = len(e1)\n",
    "    e1.sort()\n",
    "    padded_enemy_positions = e1[:max_enemies_in_local_grid]\n",
    "    num_missing = max_enemies_in_local_grid - len(padded_enemy_positions)\n",
    "    padded_enemy_positions.extend([(-1, -1)] * num_missing)\n",
    "    enemy_positions_flat = [coord for pos in padded_enemy_positions for coord in pos]\n",
    "    indices_1 = (f1, w1) + tuple(enemy_positions_flat)\n",
    "    e2=optimized_relative_positions(\n",
    "        (maze_rows, maze_cols), ((x-1)%maze_rows,y), og_enemy_positions, grid_size=grid_size\n",
    "    )\n",
    "    f2 = int(reward_maze[(x-1)%maze_rows][y] == 2)\n",
    "    w2 = int(reward_maze[(x-1)%maze_rows][y] == 1)\n",
    "    max_enemies_in_local_grid = len(e2)\n",
    "    e2.sort()\n",
    "    padded_enemy_positions = e2[:max_enemies_in_local_grid]\n",
    "    num_missing = max_enemies_in_local_grid - len(padded_enemy_positions)\n",
    "    padded_enemy_positions.extend([(-1, -1)] * num_missing)\n",
    "    enemy_positions_flat = [coord for pos in padded_enemy_positions for coord in pos]\n",
    "    indices_2 = (f2, w2) + tuple(enemy_positions_flat)\n",
    "    e3=optimized_relative_positions(\n",
    "        (maze_rows, maze_cols), (x,(y+1)%maze_rows), og_enemy_positions, grid_size=grid_size\n",
    "    )\n",
    "    f3 = int(reward_maze[x][(y+1)%maze_rows] == 2)\n",
    "    w3 = int(reward_maze[x][(y+1)%maze_rows] == 1)\n",
    "    max_enemies_in_local_grid = len(e3)\n",
    "    e3.sort()\n",
    "    padded_enemy_positions = e3[:max_enemies_in_local_grid]\n",
    "    num_missing = max_enemies_in_local_grid - len(padded_enemy_positions)\n",
    "    padded_enemy_positions.extend([(-1, -1)] * num_missing)\n",
    "    enemy_positions_flat = [coord for pos in padded_enemy_positions for coord in pos]\n",
    "    indices_3 = (f3, w3) + tuple(enemy_positions_flat)\n",
    "    e4=optimized_relative_positions(\n",
    "        (maze_rows, maze_cols), (x,(y-1)%maze_rows), og_enemy_positions, grid_size=grid_size\n",
    "    )\n",
    "    f4 = int(reward_maze[x][(y-1)%maze_rows] == 2)\n",
    "    w4 = int(reward_maze[x][(y-1)%maze_rows] == 1)\n",
    "    max_enemies_in_local_grid = len(e4)\n",
    "    e4.sort()\n",
    "    padded_enemy_positions = e4[:max_enemies_in_local_grid]\n",
    "    num_missing = max_enemies_in_local_grid - len(padded_enemy_positions)\n",
    "    padded_enemy_positions.extend([(-1, -1)] * num_missing)\n",
    "    enemy_positions_flat = [coord for pos in padded_enemy_positions for coord in pos]\n",
    "    indices_4 = (f4, w4) + tuple(enemy_positions_flat)\n",
    "    val_1 = estimated_value_grid.get(indices_1, 0)\n",
    "    val_2 = estimated_value_grid.get(indices_2, 0)\n",
    "    val_3 = estimated_value_grid.get(indices_3, 0)\n",
    "    val_4 = estimated_value_grid.get(indices_4, 0)\n",
    "\n",
    "    moves = [\n",
    "        (val_1, (1, 0)),   \n",
    "        (val_2, (-1, 0)),  \n",
    "        (val_3, (0, 1)),   \n",
    "        (val_4, (0, -1)),  \n",
    "    ]\n",
    "    QSA,learning_rate,discount,R1 =pack\n",
    "    max_future_Q = max(\n",
    "            val_1,val_2,val_3,val_4\n",
    "    )\n",
    "    estimated_value_grid[old_indices_s] = QSA + learning_rate * (R1 + discount * max_future_Q - QSA)\n",
    "    # Pick the mov\n",
    "    # e with the greatest value\n",
    "    best_val, best_move = max(moves, key=lambda x: x[0])\n",
    "    if random.random() < epsilon:\n",
    "        # Exploration: Choose a random move\n",
    "        best_move = random.choice(move_values)\n",
    "    else:\n",
    "        best_val, best_move = max(moves, key=lambda x: x[0])\n",
    "\n",
    "    policy[old_indices_s]=best_move\n",
    "\n",
    "    return policy,estimated_value_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_Policy(policy, prev_player_pos,new_player_pos,estimated_value_grid, enemies, reward_maze, visited_empty_spaces,grid_size=5,learning_rate=0.1, discount=0.9,epsilon=0.1):\n",
    "    maze_rows, maze_cols = len(reward_maze),len(reward_maze[0])\n",
    "    og_enemy_positions = [enemy[\"pos\"] for enemy in enemies]\n",
    "    new_elative_enemy_positions = optimized_relative_positions(\n",
    "        (maze_rows, maze_cols), new_player_pos, og_enemy_positions, grid_size=grid_size\n",
    "    )\n",
    "    old_relative_enemy_positions = optimized_relative_positions(\n",
    "        (maze_rows, maze_cols), prev_player_pos, og_enemy_positions, grid_size=grid_size\n",
    "    )\n",
    "    old_food = int(reward_maze[prev_player_pos[0]][prev_player_pos[1]] == 2)\n",
    "    old_wall = int(reward_maze[prev_player_pos[0]][prev_player_pos[1]] == 1)\n",
    "\n",
    "    new_food = int(reward_maze[new_player_pos[0]][new_player_pos[1]] == 2)\n",
    "    new_wall = int(reward_maze[new_player_pos[0]][new_player_pos[1]] == 1)\n",
    "\n",
    "    max_enemies_in_local_grid = len(old_relative_enemy_positions)\n",
    "    old_relative_enemy_positions.sort()\n",
    "    old_padded_enemy_positions = old_relative_enemy_positions[:max_enemies_in_local_grid]\n",
    "    num_missing = max_enemies_in_local_grid - len(old_padded_enemy_positions)\n",
    "    old_padded_enemy_positions.extend([(-1, -1)] * num_missing) \n",
    "    old_enemy_positions_flat = [coord for pos in old_padded_enemy_positions for coord in pos]\n",
    "\n",
    "\n",
    "    max_enemies_in_local_grid = len(new_elative_enemy_positions)\n",
    "    new_elative_enemy_positions.sort()\n",
    "    new_padded_enemy_positions = new_elative_enemy_positions[:max_enemies_in_local_grid]\n",
    "    num_missing = max_enemies_in_local_grid - len(new_padded_enemy_positions)\n",
    "    new_padded_enemy_positions.extend([(-1, -1)] * num_missing)  \n",
    "    new_enemy_positions_flat = [coord for pos in new_padded_enemy_positions for coord in pos]\n",
    "\n",
    "\n",
    "    old_indices_s = (old_food, old_wall) + tuple(old_enemy_positions_flat)\n",
    "    new_indices_s = (new_food, new_wall) + tuple(new_enemy_positions_flat)\n",
    "    \n",
    "    QS1A1=estimated_value_grid.get(new_indices_s)\n",
    "    if check_collision(new_player_pos, enemies):\n",
    "        R1 = -99999 # Collision with enemy\n",
    "        print(\"Collision Detected\")\n",
    "        visited_empty_spaces=0\n",
    "    elif new_wall:\n",
    "        visited_empty_spaces=0\n",
    "        R1 = -10 # Attempted to move into a wall\n",
    "    elif new_food:  # Food\n",
    "        visited_empty_spaces=0\n",
    "        R1 = 20\n",
    "    else:\n",
    "        R1 = -10 # Default penalty for empty space\n",
    "        visited_empty_spaces+=1\n",
    "    QSA=estimated_value_grid.get(old_indices_s)\n",
    "    if QSA is None:\n",
    "        QSA=0\n",
    "    if QS1A1 is None:\n",
    "        QS1A1=0\n",
    "    print(QSA,learning_rate,R1,discount,QS1A1)\n",
    "    policy,estimated_value_grid=update_policy_estimated_vals(epsilon,old_indices_s,policy, prev_player_pos,estimated_value_grid, og_enemy_positions, reward_maze,(QSA,learning_rate,discount,R1))\n",
    "    return estimated_value_grid,policy,visited_empty_spaces\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_positions(maze, num_enemies):\n",
    "    maze_rows, maze_cols = len(maze), len(maze[0])\n",
    "\n",
    "    # Find all possible positions (excluding walls)\n",
    "    possible_positions = [(x, y) for x in range(maze_rows) for y in range(maze_cols) if maze[x][y] != 1]\n",
    "\n",
    "    # Randomly select a position for the player\n",
    "    player_pos = random.choice(possible_positions)\n",
    "\n",
    "    # Remove player's position from possible positions\n",
    "    possible_positions.remove(player_pos)\n",
    "\n",
    "    enemies = []\n",
    "    for _ in range(num_enemies):\n",
    "        if not possible_positions:\n",
    "            break  # No more positions available\n",
    "        enemy_pos = random.choice(possible_positions)\n",
    "        enemies.append({\"pos\": enemy_pos, \"target\": None})\n",
    "        possible_positions.remove(enemy_pos)\n",
    "\n",
    "    return player_pos, enemies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxt=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    def __init__(self, maze, player_pos, enemies, score=0, timeout=maxt):\n",
    "        self.maze = maze\n",
    "        self.player_pos = player_pos\n",
    "        self.enemies = enemies\n",
    "        self.score = score\n",
    "        self.timeout = timeout\n",
    "        self.state_action_log = []\n",
    "        self.running = True  # Indicates if the game is still running\n",
    "\n",
    "    def restart_game(self):\n",
    "        self.maze = create_maze(ROWS, COLS)\n",
    "        self.player_pos, self.enemies = initialize_positions(self.maze, num_enemies)\n",
    "        self.score = 0\n",
    "        self.timeout = maxt\n",
    "        self.state_action_log = []\n",
    "        self.running = True\n",
    "        self.visited_empty_spaces=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reward(maze, player_pos, action):\n",
    "    x, y = player_pos\n",
    "    dx, dy = action\n",
    "\n",
    "    # Calculate the new position\n",
    "    new_x, new_y = x + dx, y + dy\n",
    "\n",
    "    # Check bounds\n",
    "    if new_x < 0 or new_x >= maze.shape[0] or new_y < 0 or new_y >= maze.shape[1]:\n",
    "        return -4, False, (x, y)  # Invalid move into a wall (out of bounds)\n",
    "\n",
    "    # Check the cell value\n",
    "    cell_value = maze[new_x, new_y]\n",
    "\n",
    "    if cell_value == 2:  # Food\n",
    "        reward = 1\n",
    "    elif cell_value == 1:  # Wall\n",
    "        reward = -4\n",
    "        return reward, False, (x, y)  # Invalid move\n",
    "    elif cell_value == 4:  # Enemy\n",
    "        reward = -100  # Game over penalty\n",
    "        return reward, False, (x, y)  # Invalid move (optional: game end logic)\n",
    "    else:  # Empty space\n",
    "        reward = 0.3\n",
    "\n",
    "    # If valid move, update the player's position\n",
    "    return reward, True, (new_x, new_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_size=64):\n",
    "        super(SimpleModel, self).__init__()    \n",
    "        self.layer = nn.Linear(input_size, 10)  # `input_size` must match flattened maze size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = create_maze(ROWS, COLS)\n",
    "player_pos, enemies = initialize_positions(maze, num_enemies)\n",
    "game = Game(maze, player_pos, enemies, score=0, timeout=maxt)\n",
    "x,y =player_pos\n",
    "maze[x][y]=3\n",
    "for enemy in enemies:\n",
    "    x,y=(enemy['pos'])\n",
    "    maze[x][y]=4   \n",
    "maze=np.array(maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_q_loss(model, target_model, maze, reward, action, next_maze, done, gamma=0.99):\n",
    "    # Flatten current and next state maze tensors\n",
    "    maze_flat = maze.view(1, -1)\n",
    "    next_maze_flat = next_maze.view(1, -1)\n",
    "\n",
    "    # Predict Q-values for the current state and the next state\n",
    "    q_values = model(maze_flat)  # Shape: [1, num_actions]\n",
    "    with torch.no_grad():\n",
    "        next_q_values = target_model(next_maze_flat)  # Shape: [1, num_actions]\n",
    "\n",
    "    # Map tuple action to an integer index\n",
    "    # Example: Define a mapping for actions\n",
    "    action_map = {\n",
    "        (0, 0): 0,  # Example mapping: \"up\" -> 0\n",
    "        (1, 0): 1,  # \"down\" -> 1\n",
    "        (0, -1): 2, # \"left\" -> 2\n",
    "        (0, 1): 3   # \"right\" -> 3\n",
    "    }\n",
    "    action_index = action_map[action]\n",
    "\n",
    "    # Get the Q-value for the specific action\n",
    "    q_value = q_values.gather(1, torch.tensor([[action_index]], dtype=torch.long)).squeeze(-1)  # Shape: [1]\n",
    "\n",
    "    # Compute the target Q-value\n",
    "    max_next_q_value = next_q_values.max(1)[0]  # Shape: [1] (Max Q-value for the next state)\n",
    "    target_q_value = torch.tensor([reward + gamma * max_next_q_value.item() * (1 - done)], dtype=torch.float32)  # Shape: [1]\n",
    "\n",
    "    # Compute Q-Loss\n",
    "    loss_fn = nn.MSELoss()\n",
    "    loss = loss_fn(q_value, target_q_value)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = torch.randn(ROWS, COLS)  # Random example maze\n",
    "next_maze = torch.randn(ROWS, COLS)  # Example next state maze\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel(input_size=ROWS * COLS)\n",
    "target_model = SimpleModel(input_size=ROWS * COLS)\n",
    "target_model.load_state_dict(model.state_dict()) \n",
    "done=0\n",
    "action = (0,0)\n",
    "reward, valid_move, new_player_pos = calculate_reward(maze, player_pos, action)\n",
    "\n",
    "# Compute Q-Loss\n",
    "loss = compute_q_loss(model, target_model, maze, reward, action, next_maze, done)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RRUN\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     39\u001b[0m game\u001b[38;5;241m.\u001b[39mtimeout \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 40\u001b[0m new_player_pos \u001b[38;5;241m=\u001b[39m \u001b[43mmove_entity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplayer_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m move_enemies(game\u001b[38;5;241m.\u001b[39menemies, game\u001b[38;5;241m.\u001b[39mmaze)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_valid_move(new_player_pos, game\u001b[38;5;241m.\u001b[39mmaze):\n",
      "File \u001b[0;32m~/Pacman_Reinforment_Learning/helper.py:187\u001b[0m, in \u001b[0;36mmove_entity\u001b[0;34m(pos, direction)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmove_entity\u001b[39m(pos, direction):\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;124;03m\"\"\"Move an entity (player or enemy) with wrap-around logic.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m     new_row \u001b[38;5;241m=\u001b[39m (pos[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[43mdirection\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m) \u001b[38;5;241m%\u001b[39m ROWS  \u001b[38;5;66;03m# Wrap around vertically\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     new_col \u001b[38;5;241m=\u001b[39m (pos[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m direction[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m%\u001b[39m COLS  \u001b[38;5;66;03m# Wrap around horizontally\u001b[39;00m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (new_row, new_col)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "pygame.init()\n",
    "print(\"RRUN\")\n",
    "global policy, maxt,estimated_value_grid\n",
    "maxt = 50\n",
    "num_games = 1  # Number of games to run\n",
    "games = []\n",
    "for _ in range(num_games):\n",
    "    maze = create_maze(ROWS, COLS)\n",
    "    player_pos, enemies = initialize_positions(maze, num_enemies)\n",
    "    game = Game(maze, player_pos, enemies, score=0, timeout=maxt)\n",
    "    x,y =player_pos\n",
    "    maze[x][y]=3\n",
    "    for enemy in enemies:\n",
    "        x,y=(enemy['pos'])\n",
    "        maze[x][y]=4    \n",
    "    games.append(game)\n",
    "\n",
    "clock = pygame.time.Clock()\n",
    "running = True\n",
    "while running:\n",
    "    # Handle events (e.g., quitting the game)\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            running = False\n",
    "\n",
    "    # For each game instance, update its state\n",
    "    all_games_ready_for_update = True\n",
    "    for game in games:\n",
    "        if not game.running:\n",
    "            continue  # Skip if game is not running\n",
    "        \n",
    "        if game.timeout == 0:\n",
    "            # Check if all games have reached the timeout\n",
    "            pass  # We'll handle synchronization after this loop\n",
    "        else:\n",
    "            all_games_ready_for_update = False\n",
    "            current_state = game.player_pos\n",
    "            action = \n",
    "            game.timeout -= 1\n",
    "            new_player_pos = move_entity(game.player_pos, action)\n",
    "            move_enemies(game.enemies, game.maze)\n",
    "            if is_valid_move(new_player_pos, game.maze):\n",
    "                if game.maze[new_player_pos[0]][new_player_pos[1]] == 2:  # Collect food\n",
    "                    game.score += 1\n",
    "                    game.maze[new_player_pos[0]][new_player_pos[1]] = 0\n",
    "                game.player_pos = new_player_pos\n",
    "            \n",
    "            # Check for collisions\n",
    "            for enemy in game.enemies:\n",
    "                if enemy[\"pos\"] == game.player_pos or check_collision(game.player_pos, game.enemies):\n",
    "                    print(\"Game Over! Restarting...\")\n",
    "                    game.running = False  # Mark the game for update\n",
    "\n",
    "            # Check if all food is collected\n",
    "            if all(game.maze[row][col] != 2 for row in range(ROWS) for col in range(COLS)):\n",
    "                print(\"You Win! Restarting...\")\n",
    "                game.running = False  # Mark the game for update\n",
    "\n",
    "            \n",
    "\n",
    "    # Synchronize updates\n",
    "    if all_games_ready_for_update or all(not game.running for game in games):\n",
    "        # Run Next_Cycle for each game\n",
    "        for game in games:\n",
    "            game.restart_game()\n",
    "            game.running = True  # Reset the running flag\n",
    "        if maxt < 10000:\n",
    "            maxt += 10\n",
    "\n",
    "    # Draw all games\n",
    "    draw_all_games(games)\n",
    "\n",
    "    pygame.display.flip()\n",
    "    clock.tick(FPS)\n",
    "pygame.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
